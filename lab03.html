<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Visualization Critique | Farduus Ibraahim</title>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <header>
        <nav>
            <a href="index.html">Home</a>
            <a href="lab02.html">Lab 2: AI Evaluation</a>
            <a href="lab03.html" class="active">Lab 3: Visualization Critique</a>
            <!-- Add more lab links as the semester progresses -->
        </nav>
        <h1>Visualization Critique</h1>
    </header>

    <main>
        <section>
            <h2>Visualization</h2>
            <figure>
                <img src="images/LLMs.png" alt="Description of the infographic" width="500">
                <figcaption>
                    <strong>Title:</strong> Major Large Language Models (LLMs)



<br>
                    <strong>Source:</strong> <a href="https://informationisbeautiful.net/visualizations/the-rise-of-generative-ai-large-language-models-llms-like-chatgpt/" target="_blank">Information is Beautiful</a>
                </figcaption>
            </figure>
        </section>

        <section>
            <h2>Critique</h2>
            
            <h3>Information Resolution</h3>
            <p>When first viewing the graph analyzing major Large Language Models, my partner and I noticed that we felt a bit overwhelmed by what was happening in the graphic. Deciphering the map is not intuitive, but it becomes more understandable as you spend time with it. There is clear analysis on how these LLMs have changed and developed over time. The form in which these LLMs’ development is being measured is Massive Multitask Language Understanding (MMLU). Which calculates the LLM’s ability to communicate, reason, and share information. Part of the reason that this graphic seems overwhelming is due to the high volume of information it aims to share. This ties directly with Tufte’s idea of resolution, where his goal is to show as much data as possible, while using clean and effective visuals. The argument here is that the human mind is able to intake large amounts of data when it is presented in a straight and direct manner.Although this graph is showing a growth in MMLU, it is only answering the question of what is happening, and gives no context as to why or how this growth is happening. The reason we are unable to answer these questions is due to the minimal qualitative data presented. Later, we are able to see more qualitative data we can join with the first chart, but even then we run into issues of trying to prove causation. </p>

            <h3>Effects Without Causes</h3>
            <p>After critiquing the visualization part of the data, the two of Tufte’s concerns that we noticed in this data visualization were effects without causes and overreaching. For the effects without causes, we noticed that the visualizations showed strong data and results, however, we noticed that it does not go into detail about how some of the data was collected. One example is the visualization below, “What are people using ChatGPT for?” Looking at it, the effect is visually persuasive, but the lack of explanation lowers the overall clarity, and it does not show us where the data has been collected from, where was it collected from, or by whom it was surveyed.  I like how in the first visualization, the line plot, each circle or diamond has an in depth articles of information with sources, but comparing some of the other visualizations, we could not get more information on it. Because of that, some of the questions we raised were how representative these percentages were. </p>
            <img src="images/IIB-uses-2552.png" alt="Description of the infographic" width="500">

            <h3>Overreaching</h3>
            <p>The other Tufte’s concern was overreaching, we noticed that one of the visualizations, “OpenAI, creators of ChatGPT, stole the LLM show,” showed that OpenAI models that performed well, however, it did not account for future uncertainty, alternative metrics, or limitations of the benchmarks themselves. One unanswered question we had was how the visualization is excellent at summarizing trends, but why it is less effective at encouraging critical evaluation of the data.</p>
             <img src="images/LLM show.png" alt="Description of the infographic" width="500">


            <h3>Conclusion</h3>
            <p>Overall, we think this visualization does a strong job of showing the scale and speed of development in the LLM space, which makes it engaging and informative. However, from a Tufte's perspective, its information resolution could be improved by better integrating causes, assumptions, and methodology directly into the visual design. Right now, the visualization excels at showing what is happening, but it asks the viewer to do much of the work in figuring out why it is happening and what that means.</p>
        </section>
    </main>

    <footer>
        <p>© 2026 Farduus Ibraahim</p>
    </footer>
</body>
</html>